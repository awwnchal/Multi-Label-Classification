{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ctk6B_XoiZC"
      },
      "source": [
        "# Overview"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " Data Loading and Preprocessing:\n",
        "   - The script reads a CSV file (`train.csv`) into a pandas DataFrame.\n",
        "   - It performs data preprocessing steps using functions defined later in the code, including converting text to lowercase, removing escape characters, HTML tags, links, digits, punctuation, stopwords, etc.\n",
        "   - The DataFrame is then filtered to keep only relevant columns (`comment_text` and labels for multi-label classification).\n",
        "\n",
        " Sample Selection:\n",
        "   - Due to the large size of the dataset, the code randomly selects 50% of the data to train the models. This reduces the data size to prevent memory issues.\n",
        "\n",
        "Word Embeddings:\n",
        "   - The script uses TF-IDF vectorization to convert the text data into numerical representations suitable for machine learning algorithms.\n",
        "   - The `TfidfVectorizer` from scikit-learn is used to perform this vectorization.\n",
        "   - The text data is split into training and testing sets using the `train_test_split` function from scikit-learn.\n",
        "\n",
        " Modelling - Binary Relevance with Different Classifiers & Classifier Chain.\n",
        "   - The script demonstrates the use of the Binary Relevance method for multi-label classification.\n",
        "   - It trains different classifiers using the MultiOutputClassifier wrapper from scikit-learn.\n",
        "   - Three classifiers are used: MultinomialNB, LogisticRegression, and DecisionTreeClassifier, Random Forest, XGBOOST.\n",
        "   - Each classifier is trained on the training data and evaluated on the testing data using metrics such as Hamming loss, accuracy, and log loss.\n",
        "\n",
        "\n",
        "   The output of each evaluation is printed to the console.\n"
      ],
      "metadata": {
        "id": "AOu6CyGeiifm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sklearn"
      ],
      "metadata": {
        "id": "uT9qC8l6ieex"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z59jm1GMqKFP",
        "outputId": "3ca2bd7d-bc5d-4a75-8ca9-ad3775a46c27"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting scikit-multilearn\n",
            "  Downloading scikit_multilearn-0.2.0-py3-none-any.whl (89 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.4/89.4 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: scikit-multilearn\n",
            "Successfully installed scikit-multilearn-0.2.0\n"
          ]
        }
      ],
      "source": [
        "!pip install scikit-multilearn\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "XYlqaYkCqLO3"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from skmultilearn.adapt import MLkNN\n",
        "from sklearn.metrics import hamming_loss, accuracy_score"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Specify the path to your CSV file\n",
        "csv_file_path = '/content/drive/MyDrive/sikka/train.csv'\n",
        "\n",
        "# Read the CSV file into a pandas DataFrame\n",
        "df = pd.read_csv(csv_file_path)"
      ],
      "metadata": {
        "id": "8ymU-4GTN0mc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 520
        },
        "id": "h8NnkpcXqa3d",
        "outputId": "7b755e7e-0ef4-4c38-eabd-86cdda620a66"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-0b5b3cd6-9807-4480-8a97-a0138b0e7e96\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>comment_text</th>\n",
              "      <th>toxic</th>\n",
              "      <th>severe_toxic</th>\n",
              "      <th>obscene</th>\n",
              "      <th>threat</th>\n",
              "      <th>insult</th>\n",
              "      <th>identity_hate</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0000997932d777bf</td>\n",
              "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>000103f0d9cfb60f</td>\n",
              "      <td>D'aww! He matches this background colour I'm s...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>000113f07ec002fd</td>\n",
              "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0001b41b1c6bb37e</td>\n",
              "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0001d958c54c6e35</td>\n",
              "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>159566</th>\n",
              "      <td>ffe987279560d7ff</td>\n",
              "      <td>\":::::And for the second time of asking, when ...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>159567</th>\n",
              "      <td>ffea4adeee384e90</td>\n",
              "      <td>You should be ashamed of yourself \\n\\nThat is ...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>159568</th>\n",
              "      <td>ffee36eab5c267c9</td>\n",
              "      <td>Spitzer \\n\\nUmm, theres no actual article for ...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>159569</th>\n",
              "      <td>fff125370e4aaaf3</td>\n",
              "      <td>And it looks like it was actually you who put ...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>159570</th>\n",
              "      <td>fff46fc426af1f9a</td>\n",
              "      <td>\"\\nAnd ... I really don't think you understand...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>159571 rows × 8 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-0b5b3cd6-9807-4480-8a97-a0138b0e7e96')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-0b5b3cd6-9807-4480-8a97-a0138b0e7e96 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-0b5b3cd6-9807-4480-8a97-a0138b0e7e96');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                      id                                       comment_text  \\\n",
              "0       0000997932d777bf  Explanation\\nWhy the edits made under my usern...   \n",
              "1       000103f0d9cfb60f  D'aww! He matches this background colour I'm s...   \n",
              "2       000113f07ec002fd  Hey man, I'm really not trying to edit war. It...   \n",
              "3       0001b41b1c6bb37e  \"\\nMore\\nI can't make any real suggestions on ...   \n",
              "4       0001d958c54c6e35  You, sir, are my hero. Any chance you remember...   \n",
              "...                  ...                                                ...   \n",
              "159566  ffe987279560d7ff  \":::::And for the second time of asking, when ...   \n",
              "159567  ffea4adeee384e90  You should be ashamed of yourself \\n\\nThat is ...   \n",
              "159568  ffee36eab5c267c9  Spitzer \\n\\nUmm, theres no actual article for ...   \n",
              "159569  fff125370e4aaaf3  And it looks like it was actually you who put ...   \n",
              "159570  fff46fc426af1f9a  \"\\nAnd ... I really don't think you understand...   \n",
              "\n",
              "        toxic  severe_toxic  obscene  threat  insult  identity_hate  \n",
              "0           0             0        0       0       0              0  \n",
              "1           0             0        0       0       0              0  \n",
              "2           0             0        0       0       0              0  \n",
              "3           0             0        0       0       0              0  \n",
              "4           0             0        0       0       0              0  \n",
              "...       ...           ...      ...     ...     ...            ...  \n",
              "159566      0             0        0       0       0              0  \n",
              "159567      0             0        0       0       0              0  \n",
              "159568      0             0        0       0       0              0  \n",
              "159569      0             0        0       0       0              0  \n",
              "159570      0             0        0       0       0              0  \n",
              "\n",
              "[159571 rows x 8 columns]"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V_brjtZgslXK",
        "outputId": "d6720dbf-07e0-4a8d-df57-19aae85e0a3b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0         Explanation\\nWhy the edits made under my usern...\n",
              "1         D'aww! He matches this background colour I'm s...\n",
              "2         Hey man, I'm really not trying to edit war. It...\n",
              "3         \"\\nMore\\nI can't make any real suggestions on ...\n",
              "4         You, sir, are my hero. Any chance you remember...\n",
              "                                ...                        \n",
              "159566    \":::::And for the second time of asking, when ...\n",
              "159567    You should be ashamed of yourself \\n\\nThat is ...\n",
              "159568    Spitzer \\n\\nUmm, theres no actual article for ...\n",
              "159569    And it looks like it was actually you who put ...\n",
              "159570    \"\\nAnd ... I really don't think you understand...\n",
              "Name: comment_text, Length: 159571, dtype: object"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "df['comment_text']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wTkIYQuZtcVG",
        "outputId": "1fb00e07-baf9-4e98-a914-165948cdc251"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(159571, 8)"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_jKmg22Kx2Im",
        "outputId": "a7c4a84b-7a28-464b-aa12-580de165c36b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "id               object\n",
              "comment_text     object\n",
              "toxic             int64\n",
              "severe_toxic      int64\n",
              "obscene           int64\n",
              "threat            int64\n",
              "insult            int64\n",
              "identity_hate     int64\n",
              "dtype: object"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.dtypes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e7pYeQTvyOQL",
        "outputId": "02124dec-3a09-4200-c41e-4b6d1130fae0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "id               159571\n",
              "comment_text     159571\n",
              "toxic                 2\n",
              "severe_toxic          2\n",
              "obscene               2\n",
              "threat                2\n",
              "insult                2\n",
              "identity_hate         2\n",
              "dtype: int64"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.nunique()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jHJsDvvxzKfq"
      },
      "source": [
        "# Problem Statement:\n",
        " A Kaggle competition for a multi-class classification problem on text data - each text sample can belong to various classes. You must create a model which predicts the probability of each class for each text sample. The details can be found here - https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge/data\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V2un26Hsz02S"
      },
      "source": [
        "# DATA PREPROCESSING"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RYS0EbJg29io",
        "outputId": "919d48d1-b7f8-4088-b138-0159634dcb2b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.7.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.0.7)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.39.3)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.4)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (23.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (8.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.0.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting venn\n",
            "  Downloading venn-0.1.3.tar.gz (19 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from venn) (3.7.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->venn) (1.0.7)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->venn) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->venn) (4.39.3)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->venn) (1.4.4)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from matplotlib->venn) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->venn) (23.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->venn) (8.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->venn) (3.0.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->venn) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->venn) (1.16.0)\n",
            "Building wheels for collected packages: venn\n",
            "  Building wheel for venn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for venn: filename=venn-0.1.3-py3-none-any.whl size=19699 sha256=a7b17f2c36e53334d90063d6f4eb345ea129342ff0b212905ad0ac4c48144325\n",
            "  Stored in directory: /root/.cache/pip/wheels/9c/ce/43/705b4a04cd822891d1d7a4c43fc444b4798978e72c79528c5f\n",
            "Successfully built venn\n",
            "Installing collected packages: venn\n",
            "Successfully installed venn-0.1.3\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting contractions\n",
            "  Downloading contractions-0.1.73-py2.py3-none-any.whl (8.7 kB)\n",
            "Collecting textsearch>=0.0.21 (from contractions)\n",
            "  Downloading textsearch-0.0.24-py2.py3-none-any.whl (7.6 kB)\n",
            "Collecting anyascii (from textsearch>=0.0.21->contractions)\n",
            "  Downloading anyascii-0.3.2-py3-none-any.whl (289 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m289.9/289.9 kB\u001b[0m \u001b[31m19.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pyahocorasick (from textsearch>=0.0.21->contractions)\n",
            "  Downloading pyahocorasick-2.0.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (110 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.8/110.8 kB\u001b[0m \u001b[31m16.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pyahocorasick, anyascii, textsearch, contractions\n",
            "Successfully installed anyascii-0.3.2 contractions-0.1.73 pyahocorasick-2.0.0 textsearch-0.0.24\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: scikit-multilearn in /usr/local/lib/python3.10/dist-packages (0.2.0)\n"
          ]
        }
      ],
      "source": [
        "# need latest version of matplotlib >=(3.4.1)\n",
        "!pip install --upgrade matplotlib\n",
        "#installing required libraries\n",
        "!pip install venn\n",
        "!pip install contractions\n",
        "!pip install scikit-multilearn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "24-DSJ493NJ6"
      },
      "outputs": [],
      "source": [
        "#mandatory libraries\n",
        "import os\n",
        "import re\n",
        "import string\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "#plotting libraries\n",
        "import venn\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "\n",
        "#NLTK libraries  & for data cleaning\n",
        "import contractions\n",
        "import nltk\n",
        "from nltk.tree import Tree\n",
        "from nltk.corpus import stopwords\n",
        "from nltk import ne_chunk, pos_tag, word_tokenize\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "from wordcloud import WordCloud, STOPWORDS\n",
        "\n",
        "#sk-learn libraries for vectorization and TSNE\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "#miscellaneous libraries\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "from tqdm.notebook import tqdm\n",
        "from itertools import combinations\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ShKrX_Rl0fnE",
        "outputId": "f344c872-0e85-40db-fe7f-34b9fa61ba51"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/words.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\"]\n",
            "[\"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his']\n",
            "['himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself']\n",
            "['they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this']\n",
            "['that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be']\n",
            "['been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing']\n",
            "['a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until']\n",
            "['while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into']\n",
            "['through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down']\n",
            "['in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once']\n",
            "['here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each']\n",
            "['few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only']\n",
            "['own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will']\n",
            "['just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o']\n",
            "['re', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\"]\n",
            "['doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\"]\n",
            "['ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn']\n",
            "[\"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n",
            "['we', 'our', 'her', 'is', \"let's\", 'herself', 'not', 'above', 'very', 'was']\n",
            "['get', 'r', 'those', 'here', 'ever', 'if', 'down', \"i've\", 'your', \"they're\"]\n",
            "['so', 'after', 'and', \"mustn't\", 'being', 'as', 'had', 'a', \"who's\", \"she'll\"]\n",
            "[\"he's\", \"you've\", 'hers', 'themselves', 'he', 'ourselves', 'than', 'nor', 'has', \"hadn't\"]\n",
            "['which', 'over', \"she's\", 'ought', \"wasn't\", 'what', 'am', 'when', 'otherwise', 'me']\n",
            "['are', 'on', 'or', 'my', 'i', 'him', \"we're\", 'under', 'this', \"why's\"]\n",
            "['the', 'it', 'until', 'few', 'from', 'too', \"shan't\", 'do', 'to', \"they've\"]\n",
            "[\"couldn't\", 'yourself', 'ours', \"you're\", \"aren't\", \"we've\", 'out', 'such', \"wouldn't\", \"we'd\"]\n",
            "['why', 'they', 'she', 'were', \"they'll\", 'himself', \"i'll\", 'before', 'therefore', 'no']\n",
            "['yourselves', 'yours', 'only', \"what's\", 'since', 'itself', 'does', \"how's\", \"it's\", 'http']\n",
            "[\"they'd\", 'again', 'be', 'cannot', 'most', 'can', 'have', 'an', 'shall', 'www']\n",
            "['during', 'theirs', 'myself', 'hence', 'of', \"haven't\", \"you'll\", 'other', 'where', 'would']\n",
            "['like', 'there', \"doesn't\", 'been', \"that's\", \"here's\", 'between', \"there's\", \"i'd\", \"shouldn't\"]\n",
            "[\"hasn't\", 'all', \"i'm\", 'by', 'did', 'who', 'also', \"he'd\", 'else', 'for']\n",
            "['however', 'you', 'then', 'against', 'in', 'com', 'these', \"weren't\", 'them', 'k']\n",
            "['whom', 'just', 'into', 'same', \"where's\", 'with', \"won't\", 'any', 'because', 'off']\n",
            "['that', 'should', \"isn't\", 'his', 'once', \"don't\", \"you'd\", \"she'd\", 'having', 'some']\n",
            "['doing', 'below', \"he'll\", 'own', \"can't\", 'could', 'its', 'at', 'while', 'their']\n",
            "[\"when's\", 'each', 'further', 'more', \"didn't\", 'but', 'through', 'up', 'how', 'both']\n",
            "['about', \"we'll\"]\n",
            "['her', \"let's\", 're', 'mightn', 'above', 'mustn', 'ain', 'very', 'r', 'wasn']\n",
            "['those', 'here', 'if', \"should've\", 'needn', \"they're\", \"that'll\", 'and', 'had', 'a']\n",
            "[\"he's\", 'hers', 'has', 'over', 'am', 'me', 'are', 'hadn', 'on', 'my']\n",
            "[\"why's\", \"we're\", 'under', 'few', 'won', \"they've\", 'ours', \"we'd\", 'why', 'they']\n",
            "[\"they'll\", 'only', 'itself', 'does', \"how's\", \"it's\", \"they'd\", 'again', 'be', 'cannot']\n",
            "['most', 'www', 'have', 'will', 'during', 'theirs', 'hence', 'of', 'where', 'like']\n",
            "[\"doesn't\", 'aren', 'ma', \"that's\", 'between', \"there's\", \"shouldn't\", \"hasn't\", 'also', \"he'd\"]\n",
            "['else', 'how', 'for', 'however', 'you', 'then', 'com', 'them', 'whom', 'same']\n",
            "[\"where's\", 'with', 'any', 'that', 'should', \"isn't\", 'some', 'below', \"can't\", 'its']\n",
            "[\"when's\", 'each', \"didn't\", 'up', 'in', 'both', \"we'll\", 'we', 'our', 'is']\n",
            "['herself', 'not', 'couldn', 'wouldn', 'was', 'shan', 'get', 'ever', 'down', \"i've\"]\n",
            "['your', 'so', 'after', \"mustn't\", 'being', 'haven', 'as', \"who's\", \"she'll\", 'now']\n",
            "[\"you've\", 'themselves', 'he', 'ourselves', 'than', 'doesn', 'nor', 'which', \"hadn't\", 'y']\n",
            "['don', \"she's\", 'm', 'ought', \"wasn't\", 'what', 'when', 'otherwise', 'or', 'i']\n",
            "['him', 'this', 'the', 'it', 'until', 'didn', 'from', 'too', \"shan't\", 'do']\n",
            "['to', \"couldn't\", 'yourself', \"you're\", \"aren't\", 's', \"we've\", 'out', 'such', \"wouldn't\"]\n",
            "['isn', 'she', 'were', 'himself', \"i'll\", 'before', 'therefore', 'no', 'yourselves', 'yours']\n",
            "[\"what's\", 'since', 'o', 'http', 'can', 'an', 'shall', \"mightn't\", 'hasn', 'myself']\n",
            "[\"haven't\", \"you'll\", 'other', 'would', 'there', 'been', \"here's\", \"i'd\", 'all', \"i'm\"]\n",
            "['by', 'did', 'who', 'against', 'll', 'these', \"weren't\", 'just', 'into', \"won't\"]\n",
            "['shouldn', 'because', 'off', 'his', 'once', \"don't\", \"you'd\", \"she'd\", 'having', 'doing']\n",
            "[\"he'll\", \"needn't\", 'own', 'could', 'at', 'while', 'weren', 'their', 'd', 've']\n",
            "['further', 'more', 'through', 'but', 'k', 't', 'about', 'mr', 'mrs', 'miss']\n",
            "['one', 'two', 'three', 'four', 'five', 'six', 'seven', 'eight', 'nine', 'ten']\n",
            "['us', 'also', 'dont', 'cant', 'any', 'can', 'along', 'among', 'during', 'anyone']\n",
            "['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j']\n",
            "['k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't']\n",
            "['u', 'v', 'w', 'x', 'y', 'z', 'hi', 'hello', 'hey', 'ok']\n",
            "['okay', 'lol', 'rofl', 'hola', 'let', 'may', 'etc']\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "import string\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tree import Tree\n",
        "from nltk.chunk import ne_chunk\n",
        "from nltk.tokenize import word_tokenize\n",
        "import contractions\n",
        "from tqdm import tqdm\n",
        "from wordcloud import STOPWORDS\n",
        "import numpy as np\n",
        "\n",
        "# Function to convert the input text to lower case\n",
        "def convert_to_lower_case(text):\n",
        "    return text.lower()\n",
        "\n",
        "# Function to remove newline, tab, and slashes from the input text\n",
        "def remove_escape_char(text):\n",
        "    return re.sub(r\"[\\n\\t\\\\\\/]\", \" \", text, flags=re.MULTILINE)\n",
        "\n",
        "# Function to remove HTML tags and its content from the input text\n",
        "def remove_html_tags(text):\n",
        "    return re.sub(r\"<.*>\", \" \", text, flags=re.MULTILINE)\n",
        "\n",
        "# Function to remove any kind of links without HTML tags\n",
        "def remove_links(text):\n",
        "    text = re.sub(r\"http\\S+\", \" \", text, flags=re.MULTILINE)\n",
        "    return re.sub(r\"www\\S+\", \" \", text, flags=re.MULTILINE)\n",
        "\n",
        "# Function to remove digits from the input text\n",
        "def remove_digits(text):\n",
        "    return re.sub(r'\\d', \" \", text, flags=re.MULTILINE)\n",
        "\n",
        "# Function to remove punctuation marks from the input text\n",
        "def remove_punctuation(text):\n",
        "    for i in string.punctuation:\n",
        "        text = text.replace(i, \" \")\n",
        "    return text\n",
        "\n",
        "# Function to keep only alphabets and underscore\n",
        "def keep_alpha_and_underscore(text):\n",
        "    return re.sub(r\"[^a-zA-Z_]\", \" \", text, flags=re.MULTILINE)\n",
        "\n",
        "# Function to remove extra spaces if any\n",
        "def remove_extra_spaces_if_any(text):\n",
        "    return re.sub(r\" {2,}\", \" \", text, flags=re.MULTILINE)\n",
        "\n",
        "# Downloading necessary NLTK resources\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Printing stop words from NLTK library\n",
        "stop_words = stopwords.words('english')\n",
        "display_length = 10\n",
        "for i in range(int(np.ceil(len(stop_words) / display_length))):\n",
        "    print(stop_words[i * display_length:(i + 1) * display_length])\n",
        "\n",
        "# Printing stop words from Word Cloud library\n",
        "display_length = 10\n",
        "word_cloud_stp_wrds = list(STOPWORDS)\n",
        "for i in range(int(np.ceil(len(list(word_cloud_stp_wrds)) / display_length))):\n",
        "    print(word_cloud_stp_wrds[i * display_length:(i + 1) * display_length])\n",
        "\n",
        "# Creating a list of final stop words by combining NLTK and Word Cloud stop words,\n",
        "# and adding custom words\n",
        "final_stop_words = list(STOPWORDS.union(set(stop_words)))\n",
        "final_stop_words.extend([\"mr\", \"mrs\", \"miss\", \"one\", \"two\", \"three\", \"four\", \"five\",\n",
        "                         \"six\", \"seven\", \"eight\", \"nine\", \"ten\", \"us\", \"also\", \"dont\", \"cant\",\n",
        "                         \"any\", \"can\", \"along\", \"among\", \"during\", \"anyone\", \"a\", \"b\", \"c\",\n",
        "                         \"d\", \"e\", \"f\", \"g\", \"h\", \"i\", \"j\", \"k\", \"l\", \"m\", \"n\", \"o\", \"p\", \"q\", \"r\",\n",
        "                         \"s\", \"t\", \"u\", \"v\", \"w\", \"x\", \"y\", \"z\", \"hi\", \"hello\", \"hey\", \"ok\", \"okay\",\n",
        "                         \"lol\", \"rofl\", \"hola\", \"let\", \"may\", \"etc\"])\n",
        "display_length = 10\n",
        "for i in range(int(np.ceil(len(final_stop_words) / display_length))):\n",
        "    print(final_stop_words[i * display_length:(i + 1) * display_length])\n",
        "\n",
        "lemmatiser = WordNetLemmatizer()\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qtwiApmRLTcB"
      },
      "outputs": [],
      "source": [
        "# Preprocessing function\n",
        "def preprocess(text):\n",
        "    preprocessed_text = []\n",
        "    for each_text in tqdm(text):\n",
        "        result = remove_links(each_text)\n",
        "        result = remove_html_tags(result)\n",
        "        result = remove_escape_char(result)\n",
        "        result = remove_digits(result)\n",
        "        result = remove_punctuation(result)\n",
        "        result = convert_to_lower_case(result)\n",
        "        result = ' '.join(non_stop_word for non_stop_word in result.split() if non_stop_word not in final_stop_words)\n",
        "        result = keep_alpha_and_underscore(result)\n",
        "        result = remove_extra_spaces_if_any(result)\n",
        "        result = ' '.join(lemmatiser.lemmatize(word, pos=\"v\") for word in result.split())\n",
        "        preprocessed_text.append(result.strip())\n",
        "    return preprocessed_text\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RC8M68P80fpP",
        "outputId": "e263051c-f614-4dde-b04d-7d042789e30c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 159571/159571 [47:23<00:00, 56.12it/s]\n"
          ]
        }
      ],
      "source": [
        "# Performing the preprocessing on all the comments in the dataset\n",
        "preprocessed_data = preprocess(df['comment_text'].values)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "__ukJ80UjqU3"
      },
      "outputs": [],
      "source": [
        "df['comment_text']= preprocessed_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xlDMlyUoCVCK"
      },
      "outputs": [],
      "source": [
        "df.to_csv(\"clean_comments.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "lpk6GO0wKkgd"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(\"/content/clean_comments.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 624
        },
        "id": "PLhwO6zyLC_Q",
        "outputId": "32b35ac5-7d32-43dc-dc9f-6bb2fa228a78"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "        Unnamed: 0                id  \\\n",
              "0                0  0000997932d777bf   \n",
              "1                1  000103f0d9cfb60f   \n",
              "2                2  000113f07ec002fd   \n",
              "3                3  0001b41b1c6bb37e   \n",
              "4                4  0001d958c54c6e35   \n",
              "...            ...               ...   \n",
              "159566      159566  ffe987279560d7ff   \n",
              "159567      159567  ffea4adeee384e90   \n",
              "159568      159568  ffee36eab5c267c9   \n",
              "159569      159569  fff125370e4aaaf3   \n",
              "159570      159570  fff46fc426af1f9a   \n",
              "\n",
              "                                             comment_text  toxic  \\\n",
              "0       explanation edit make username hardcore metall...      0   \n",
              "1       aww match background colour seemingly stick th...      0   \n",
              "2       man really try edit war guy constantly remove ...      0   \n",
              "3       make real suggestions improvement wonder secti...      0   \n",
              "4                           sir hero chance remember page      0   \n",
              "...                                                   ...    ...   \n",
              "159566  second time ask view completely contradict cov...      0   \n",
              "159567               ashamed horrible thing put talk page      0   \n",
              "159568               umm actual article prostitution ring      0   \n",
              "159569  look actually put speedy first version delete ...      0   \n",
              "159570  really think understand come idea bad right aw...      0   \n",
              "\n",
              "        severe_toxic  obscene  threat  insult  identity_hate  \n",
              "0                  0        0       0       0              0  \n",
              "1                  0        0       0       0              0  \n",
              "2                  0        0       0       0              0  \n",
              "3                  0        0       0       0              0  \n",
              "4                  0        0       0       0              0  \n",
              "...              ...      ...     ...     ...            ...  \n",
              "159566             0        0       0       0              0  \n",
              "159567             0        0       0       0              0  \n",
              "159568             0        0       0       0              0  \n",
              "159569             0        0       0       0              0  \n",
              "159570             0        0       0       0              0  \n",
              "\n",
              "[159571 rows x 9 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-69f967d5-a1d4-47ce-9269-d11ffcc550a2\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>id</th>\n",
              "      <th>comment_text</th>\n",
              "      <th>toxic</th>\n",
              "      <th>severe_toxic</th>\n",
              "      <th>obscene</th>\n",
              "      <th>threat</th>\n",
              "      <th>insult</th>\n",
              "      <th>identity_hate</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0000997932d777bf</td>\n",
              "      <td>explanation edit make username hardcore metall...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>000103f0d9cfb60f</td>\n",
              "      <td>aww match background colour seemingly stick th...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>000113f07ec002fd</td>\n",
              "      <td>man really try edit war guy constantly remove ...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>0001b41b1c6bb37e</td>\n",
              "      <td>make real suggestions improvement wonder secti...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>0001d958c54c6e35</td>\n",
              "      <td>sir hero chance remember page</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>159566</th>\n",
              "      <td>159566</td>\n",
              "      <td>ffe987279560d7ff</td>\n",
              "      <td>second time ask view completely contradict cov...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>159567</th>\n",
              "      <td>159567</td>\n",
              "      <td>ffea4adeee384e90</td>\n",
              "      <td>ashamed horrible thing put talk page</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>159568</th>\n",
              "      <td>159568</td>\n",
              "      <td>ffee36eab5c267c9</td>\n",
              "      <td>umm actual article prostitution ring</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>159569</th>\n",
              "      <td>159569</td>\n",
              "      <td>fff125370e4aaaf3</td>\n",
              "      <td>look actually put speedy first version delete ...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>159570</th>\n",
              "      <td>159570</td>\n",
              "      <td>fff46fc426af1f9a</td>\n",
              "      <td>really think understand come idea bad right aw...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>159571 rows × 9 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-69f967d5-a1d4-47ce-9269-d11ffcc550a2')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-69f967d5-a1d4-47ce-9269-d11ffcc550a2 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-69f967d5-a1d4-47ce-9269-d11ffcc550a2');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ],
      "source": [
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "sxbfcyKRLKeA"
      },
      "outputs": [],
      "source": [
        "df = df[['comment_text', 'toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GpHAx3qCD53J",
        "outputId": "37193842-d3fb-45ef-ec55-b8f05057b083"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "df['comment_text'].isnull().values.any()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vLB8vnCBQxWN"
      },
      "source": [
        "after cleaning, some comments became nan\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "nKQDq3T4Lw4w"
      },
      "outputs": [],
      "source": [
        "df = df.dropna(subset=['comment_text'])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gS_r4iYmNn7J"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "b6lyb5QDNpGN",
        "outputId": "5c36e3fb-9291-4ace-ae86-39c8d93789ff"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-b828f40e-b529-45b7-b275-082a5b480f3d\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>comment_text</th>\n",
              "      <th>toxic</th>\n",
              "      <th>severe_toxic</th>\n",
              "      <th>obscene</th>\n",
              "      <th>threat</th>\n",
              "      <th>insult</th>\n",
              "      <th>identity_hate</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>explanation edit make username hardcore metall...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>aww match background colour seemingly stick th...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>man really try edit war guy constantly remove ...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>make real suggestions improvement wonder secti...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>sir hero chance remember page</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>159566</th>\n",
              "      <td>second time ask view completely contradict cov...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>159567</th>\n",
              "      <td>ashamed horrible thing put talk page</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>159568</th>\n",
              "      <td>umm actual article prostitution ring</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>159569</th>\n",
              "      <td>look actually put speedy first version delete ...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>159570</th>\n",
              "      <td>really think understand come idea bad right aw...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>159190 rows × 7 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b828f40e-b529-45b7-b275-082a5b480f3d')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-b828f40e-b529-45b7-b275-082a5b480f3d button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-b828f40e-b529-45b7-b275-082a5b480f3d');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                                             comment_text  toxic  \\\n",
              "0       explanation edit make username hardcore metall...      0   \n",
              "1       aww match background colour seemingly stick th...      0   \n",
              "2       man really try edit war guy constantly remove ...      0   \n",
              "3       make real suggestions improvement wonder secti...      0   \n",
              "4                           sir hero chance remember page      0   \n",
              "...                                                   ...    ...   \n",
              "159566  second time ask view completely contradict cov...      0   \n",
              "159567               ashamed horrible thing put talk page      0   \n",
              "159568               umm actual article prostitution ring      0   \n",
              "159569  look actually put speedy first version delete ...      0   \n",
              "159570  really think understand come idea bad right aw...      0   \n",
              "\n",
              "        severe_toxic  obscene  threat  insult  identity_hate  \n",
              "0                  0        0       0       0              0  \n",
              "1                  0        0       0       0              0  \n",
              "2                  0        0       0       0              0  \n",
              "3                  0        0       0       0              0  \n",
              "4                  0        0       0       0              0  \n",
              "...              ...      ...     ...     ...            ...  \n",
              "159566             0        0       0       0              0  \n",
              "159567             0        0       0       0              0  \n",
              "159568             0        0       0       0              0  \n",
              "159569             0        0       0       0              0  \n",
              "159570             0        0       0       0              0  \n",
              "\n",
              "[159190 rows x 7 columns]"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XegZnGcDWIZe"
      },
      "source": [
        "# The data set is too large resulting in colab notebook crashing multiple times hence I'm randomly selecting 50 % of the data and will train my models on this sample"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "U8gp9OxWWPoi"
      },
      "outputs": [],
      "source": [
        "sample_frac = 0.50 # Specify the desired fraction of the dataset\n",
        "\n",
        "df = df.sample(frac=sample_frac, random_state=42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "MVpysHWIRBUp"
      },
      "outputs": [],
      "source": [
        "#mandatory libraries\n",
        "import os\n",
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import scipy\n",
        "import string\n",
        "\n",
        "#nltk-preprocessing\n",
        "import nltk\n",
        "from wordcloud import WordCloud, STOPWORDS\n",
        "from nltk.corpus import stopwords\n",
        "from nltk import ne_chunk, pos_tag, word_tokenize\n",
        "from nltk.tree import Tree\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "\n",
        "#plotting\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "#misc\n",
        "import joblib\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "from tqdm.notebook import tqdm\n",
        "from itertools import combinations\n",
        "\n",
        "#multi-processing\n",
        "import multiprocessing\n",
        "from multiprocessing import Pool,freeze_support\n",
        "from multiprocessing import Process\n",
        "\n",
        "#multi-label\n",
        "from skmultilearn.model_selection import iterative_train_test_split\n",
        "from skmultilearn.problem_transform import BinaryRelevance\n",
        "from skmultilearn.problem_transform import ClassifierChain\n",
        "from skmultilearn.problem_transform import LabelPowerset\n",
        "\n",
        "#metrics\n",
        "from sklearn.metrics import hamming_loss\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import log_loss\n",
        "from sklearn.metrics import roc_curve, auc,roc_auc_score\n",
        "\n",
        "#modelling\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "#Tensor flow for NLP\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Dense,Input,Activation,Dropout,BatchNormalization\n",
        "from tensorflow.keras.models import Model,Sequential\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from tensorflow.keras.callbacks import LearningRateScheduler\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
        "from keras.callbacks import Callback\n",
        "\n",
        "#model loading\n",
        "from tensorflow.keras.models import load_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "6ybQYQNgNqqu"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.multioutput import MultiOutputClassifier\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.model_selection import train_test_split\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "jk_XpewcNquz"
      },
      "outputs": [],
      "source": [
        "X = df['comment_text']\n",
        "y = df[['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e3Op1Q2PRREx"
      },
      "source": [
        "# Train test split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "3hGf-glkNqxB"
      },
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "08fJ_PY3RTAg"
      },
      "source": [
        "# Word embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "DICvFjWtNqzo"
      },
      "outputs": [],
      "source": [
        "vectorizer = TfidfVectorizer()\n",
        "X_train = vectorizer.fit_transform(X_train)\n",
        "X_test = vectorizer.transform(X_test)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UlB7s0zlRXxN"
      },
      "source": [
        "# Modelling\n",
        "\n",
        "Solving a multi-label classification problem is not straight forward. We dont have a straight away algorithm or classifier, but we can do probelm transformation and we can do \"adapted\" algorithms.\n",
        "\n",
        "Different approaches to solve a multi-label classification problem, namely:\n",
        "\n",
        "1. Problem Transformation\n",
        "\n",
        "2. Adapted Algorithm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ROnKL6WYR7Is",
        "outputId": "dcc7eebe-2ef4-434d-a3f3-0a9eaf48dfc8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: scikit-multilearn in /usr/local/lib/python3.10/dist-packages (0.2.0)\n"
          ]
        }
      ],
      "source": [
        "# need scikit-multilearn library for multi-label classification\n",
        "!pip install scikit-multilearn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3wLkcNMAR6aU"
      },
      "source": [
        "# Problem Transformation Methods :\n",
        "These include the Binary Relevance, Label Powerset and Classifier Chain methods.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z7olydkMSSWL"
      },
      "source": [
        "# Binary Relevance - MultinomialNB\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118
        },
        "id": "SNToelduN3fK",
        "outputId": "d3d15620-97a8-4efe-df97-b6b028688927"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MultiOutputClassifier(estimator=MultinomialNB())</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MultiOutputClassifier</label><div class=\"sk-toggleable__content\"><pre>MultiOutputClassifier(estimator=MultinomialNB())</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: MultinomialNB</label><div class=\"sk-toggleable__content\"><pre>MultinomialNB()</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MultinomialNB</label><div class=\"sk-toggleable__content\"><pre>MultinomialNB()</pre></div></div></div></div></div></div></div></div></div></div>"
            ],
            "text/plain": [
              "MultiOutputClassifier(estimator=MultinomialNB())"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Train a single MultinomialNB classifier\n",
        "classifier = MultinomialNB()\n",
        "\n",
        "# Wrap the classifier with MultiOutputClassifier\n",
        "br_classifier = MultiOutputClassifier(classifier)\n",
        "\n",
        "# Fit the classifier to the training data\n",
        "br_classifier.fit(X_train, y_train)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4z7R4SuoOrsN",
        "outputId": "fc6dac12-3aa9-4cd5-aa6f-cf1ad3081ebf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Hamming Loss: 0.03240159128978225\n",
            "Accuracy: 0.8989740368509213\n",
            "Log Loss: 0.3895355847069237\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import hamming_loss, accuracy_score, log_loss\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "\n",
        "# Convert the y_train to binary format\n",
        "y_train_binary = y_train.values.astype(int)\n",
        "\n",
        "# Train the OneVsRestClassifier with MultinomialNB\n",
        "br_classifier = OneVsRestClassifier(MultinomialNB())\n",
        "br_classifier.fit(X_train, y_train_binary)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred_binary = br_classifier.predict(X_test)\n",
        "y_pred_prob= br_classifier.predict_proba(X_test)\n",
        "# Calculate Hamming loss\n",
        "hamming_loss_value = hamming_loss(y_test, y_pred_binary)\n",
        "print(\"Hamming Loss:\", hamming_loss_value)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred_binary)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "\n",
        "# Calculate log loss\n",
        "log_loss_value = log_loss(y_test, y_pred_prob)\n",
        "print(\"Log Loss:\", log_loss_value)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kg0fzLxLQU9Z"
      },
      "source": [
        "\n",
        "\n",
        "1. Hamming Loss: 0.03\n",
        "   - This indicates that, on average, the model misclassifies around 3% of the labels. A lower Hamming loss is desirable, so the obtained value suggests that the model performs well in terms of label-wise accuracy.\n",
        "\n",
        "2. Accuracy: 0.89\n",
        "   - The accuracy of approximately 89% indicates that the model correctly predicts around 89.8% of the labels in the test set. It's important to note that accuracy alone might not be sufficient to evaluate the performance of a multi-label classification model, especially when dealing with imbalanced datasets or varying importance of different labels.\n",
        "\n",
        "3. Log Loss: 0.38\n",
        "   - Log loss measures the discrepancy between the predicted probabilities and the true labels. A lower log loss value indicates better alignment between the predicted probabilities and the true labels."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TR5jfg9edcOl"
      },
      "source": [
        "# Binary Relevance - LogisticRegression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9FDg-14O7yaO",
        "outputId": "558ce1f6-15c4-4f93-f2cb-3306d465d94d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LogisticRegression - Hamming Loss: 0.021496370742601897\n",
            "LogisticRegression - Accuracy: 0.9141541038525963\n",
            "LogisticRegression - Log Loss: 0.2842339309438655\n"
          ]
        }
      ],
      "source": [
        "# LogisticRegression\n",
        "classifier = LogisticRegression()\n",
        "br_classifier = MultiOutputClassifier(classifier)\n",
        "br_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Predict probabilities using each classifier\n",
        "y_pred_proba = [clf.predict_proba(X_test)[:, 1] for clf in br_classifier.estimators_]\n",
        "\n",
        "# Reshape predicted probabilities\n",
        "y_pred_proba_reshaped = np.array(y_pred_proba).T\n",
        "\n",
        "hamming_loss_value = hamming_loss(y_test, y_pred_binary)\n",
        "accuracy = accuracy_score(y_test, y_pred_binary)\n",
        "log_loss_value = log_loss(y_test, y_pred_proba_reshaped)\n",
        "print(\"LogisticRegression - Hamming Loss:\", hamming_loss_value)\n",
        "print(\"LogisticRegression - Accuracy:\", accuracy)\n",
        "print(\"LogisticRegression - Log Loss:\", log_loss_value)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6uBQloc2dgn1"
      },
      "source": [
        "# Binary Relevance - DecisionTreeClassifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1bNQRT_U81zp",
        "outputId": "efef4ca6-e424-456e-9837-78ad4b4bedfd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DecisionTreeClassifier - Hamming Loss: 0.02568397543271915\n",
            "DecisionTreeClassifier - Accuracy: 0.8885050251256281\n",
            "DecisionTreeClassifier - Log Loss: 1.695421880141967\n"
          ]
        }
      ],
      "source": [
        "# DecisionTreeClassifier\n",
        "classifier = DecisionTreeClassifier()\n",
        "br_classifier = MultiOutputClassifier(classifier)\n",
        "br_classifier.fit(X_train, y_train)\n",
        "\n",
        "y_pred_binary = br_classifier.predict(X_test)\n",
        "\n",
        "y_pred_proba = [clf.predict_proba(X_test)[:, 1] for clf in br_classifier.estimators_]\n",
        "# Reshape predicted probabilities\n",
        "y_pred_proba_reshaped = np.array(y_pred_proba).T\n",
        "\n",
        "hamming_loss_value = hamming_loss(y_test, y_pred_binary)\n",
        "accuracy = accuracy_score(y_test, y_pred_binary)\n",
        "log_loss_value = log_loss(y_test, y_pred_proba_reshaped)\n",
        "\n",
        "print(\"DecisionTreeClassifier - Hamming Loss:\", hamming_loss_value)\n",
        "print(\"DecisionTreeClassifier - Accuracy:\", accuracy)\n",
        "print(\"DecisionTreeClassifier - Log Loss:\", log_loss_value)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-JFUT4w3dlqs"
      },
      "source": [
        "# Binary Relevance - RF\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dHFABIpJAr-V",
        "outputId": "9d76651d-283c-4f03-f69b-641157de1dd0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "RandomForestClassifier - Hamming Loss: 0.020292434394193187\n",
            "RandomForestClassifier - Accuracy: 0.9135259631490787\n",
            "RandomForestClassifier - Log Loss: 0.3965953845384737\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# RandomForestClassifier\n",
        "classifier = RandomForestClassifier()\n",
        "br_classifier = MultiOutputClassifier(classifier)\n",
        "br_classifier.fit(X_train, y_train)\n",
        "y_pred_binary = br_classifier.predict(X_test)\n",
        "y_pred_proba = [clf.predict_proba(X_test)[:, 1] for clf in br_classifier.estimators_]\n",
        "# Reshape predicted probabilities\n",
        "y_pred_proba_reshaped = np.array(y_pred_proba).T\n",
        "\n",
        "hamming_loss_value = hamming_loss(y_test, y_pred_binary)\n",
        "accuracy = accuracy_score(y_test, y_pred_binary)\n",
        "log_loss_value = log_loss(y_test, y_pred_proba_reshaped)\n",
        "print(\"RandomForestClassifier - Hamming Loss:\", hamming_loss_value)\n",
        "print(\"RandomForestClassifier - Accuracy:\", accuracy)\n",
        "print(\"RandomForestClassifier - Log Loss:\", log_loss_value)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ed9G7EUaAb9M"
      },
      "source": [
        "# Binary Relevance - XGB\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MwuA9wBaDXc9",
        "outputId": "5c6675e3-892d-43f3-fa8c-5ce20e8b7ee5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "XGBClassifier - Hamming Loss: 0.02065884980457845\n",
            "XGBClassifier - Accuracy: 0.91321189279732\n",
            "XGBClassifier - Log Loss: 0.2924138137729429\n"
          ]
        }
      ],
      "source": [
        "# XGBClassifier\n",
        "classifier = XGBClassifier()\n",
        "br_classifier = MultiOutputClassifier(classifier)\n",
        "br_classifier.fit(X_train, y_train)\n",
        "y_pred_binary = br_classifier.predict(X_test)\n",
        "\n",
        "y_pred_proba = [clf.predict_proba(X_test)[:, 1] for clf in br_classifier.estimators_]\n",
        "# Reshape predicted probabilities\n",
        "y_pred_proba_reshaped = np.array(y_pred_proba).T\n",
        "\n",
        "hamming_loss_value = hamming_loss(y_test, y_pred_binary)\n",
        "accuracy = accuracy_score(y_test, y_pred_binary)\n",
        "log_loss_value = log_loss(y_test, y_pred_proba_reshaped)\n",
        "print(\"XGBClassifier - Hamming Loss:\", hamming_loss_value)\n",
        "print(\"XGBClassifier - Accuracy:\", accuracy)\n",
        "print(\"XGBClassifier - Log Loss:\", log_loss_value)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Among these models, both XGBOOST and LogisticRegression have similar performance in terms of Hamming Loss and Accuracy.\n",
        "\n"
      ],
      "metadata": {
        "id": "Q4DMmIa-jU66"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KyP6DTPphwMp"
      },
      "source": [
        "# Binary Relevance method  does not take into account the interdependence of labels and basically creates a separate classifier for each of the labels."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rn-lIK0bh1Q3"
      },
      "source": [
        "# Classifer chain\n",
        "\n",
        "Classifier Chains is another simple technique, unlike Binary Relevance Classifier Chain preserves the relationship between the features. The operation of classifier chain works as follows...\n",
        "\n",
        "classifier-1 will takes all the inputs and fits on the first target labels alone and the classifier-2 takes all the inputs and the first target labels together and fit on the 2nd label. Classifier-3 takes all the inputs and the first, second target labels all together as input and fits on the 3rd target label. and so on\n",
        "\n",
        "Generalizing the folow as the first classifier is trained just on the input data and then each next classifier is trained on the input space and all the previous classifiers in the chain."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "15FSJN-i1YA3",
        "outputId": "8ade3847-d026-46c0-c511-3f3a83bbf15e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Hamming Loss (MultinomialNB): 0.030220547180346176\n",
            "Log Loss (MultinomialNB): 0.40508569313919496\n",
            "Accuracy (MultinomialNB): 0.9027428810720268\n"
          ]
        }
      ],
      "source": [
        "# Multinomial Naive Bayes\n",
        "cc_classifier_mnb = ClassifierChain(classifier=MultinomialNB())\n",
        "cc_classifier_mnb.fit(X_train, y_train)\n",
        "\n",
        "y_pred_mnb = cc_classifier_mnb.predict(X_test)\n",
        "hamming_loss_mnb = hamming_loss(y_test, y_pred_mnb)\n",
        "\n",
        "y_pred= cc_classifier_mnb.predict_proba(X_test)\n",
        "\n",
        "log_loss_mnb = log_loss(y_test, y_pred.toarray())\n",
        "accuracy_mnb = accuracy_score(y_test, y_pred_mnb)\n",
        "\n",
        "print(\"Hamming Loss (MultinomialNB):\", hamming_loss_mnb)\n",
        "print(\"Log Loss (MultinomialNB):\", log_loss_mnb)\n",
        "print(\"Accuracy (MultinomialNB):\", accuracy_mnb)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QyqL5WVJOdTN"
      },
      "outputs": [],
      "source": [
        "# Logistic Regression\n",
        "from skmultilearn.problem_transform import ClassifierChain\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "# initialize classifier chains multi-label classifier\n",
        "classifier = ClassifierChain(LogisticRegression())\n",
        "# Training logistic regression model on train data\n",
        "classifier.fit(X_train, y_train)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oSX8R_C1OL1q",
        "outputId": "b78a2d3c-9174-4fd0-d7bb-c2538eac1a7f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Hamming Loss : 0.023345896147403684\n",
            "Log Loss : 0.3250033568561115\n",
            "Accuracy: 0.9104899497487438\n"
          ]
        }
      ],
      "source": [
        "\n",
        "y_pred= classifier.predict(X_test)\n",
        "y_pred_prob= classifier.predict_proba(X_test)\n",
        "\n",
        "hamming_loss_mnb = hamming_loss(y_test, y_pred)\n",
        "log_loss_mnb = log_loss(y_test, y_pred_prob.toarray())\n",
        "accuracy_mnb = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(\"Hamming Loss :\", hamming_loss_mnb)\n",
        "print(\"Log Loss :\", log_loss_mnb)\n",
        "print(\"Accuracy:\", accuracy_mnb)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#XGB\n",
        "from skmultilearn.problem_transform import ClassifierChain\n",
        "from sklearn.metrics import hamming_loss, log_loss, accuracy_score\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "# Create ClassifierChain with XGBoost classifier\n",
        "cc_classifier_xgb = ClassifierChain(classifier=XGBClassifier())\n",
        "\n",
        "# Train the classifier\n",
        "cc_classifier_xgb.fit(X_train, y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118
        },
        "id": "swikXScgeCtb",
        "outputId": "01d4293e-5563-4636-918f-7222681e541f"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ClassifierChain(classifier=XGBClassifier(base_score=None, booster=None,\n",
              "                                         callbacks=None, colsample_bylevel=None,\n",
              "                                         colsample_bynode=None,\n",
              "                                         colsample_bytree=None,\n",
              "                                         early_stopping_rounds=None,\n",
              "                                         enable_categorical=False,\n",
              "                                         eval_metric=None, feature_types=None,\n",
              "                                         gamma=None, gpu_id=None,\n",
              "                                         grow_policy=None, importance_type=None,\n",
              "                                         interaction_constraints=None,\n",
              "                                         learning_rate=None, max_bin=None,\n",
              "                                         max_cat_threshold=None,\n",
              "                                         max_cat_to_onehot=None,\n",
              "                                         max_delta_step=None, max_depth=None,\n",
              "                                         max_leaves=None, min_child_weight=None,\n",
              "                                         missing=nan, monotone_constraints=None,\n",
              "                                         n_estimators=100, n_jobs=None,\n",
              "                                         num_parallel_tree=None, predictor=None,\n",
              "                                         random_state=None, ...),\n",
              "                require_dense=[True, True])"
            ],
            "text/html": [
              "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>ClassifierChain(classifier=XGBClassifier(base_score=None, booster=None,\n",
              "                                         callbacks=None, colsample_bylevel=None,\n",
              "                                         colsample_bynode=None,\n",
              "                                         colsample_bytree=None,\n",
              "                                         early_stopping_rounds=None,\n",
              "                                         enable_categorical=False,\n",
              "                                         eval_metric=None, feature_types=None,\n",
              "                                         gamma=None, gpu_id=None,\n",
              "                                         grow_policy=None, importance_type=None,\n",
              "                                         interaction_constraints=None,\n",
              "                                         learning_rate=None, max_bin=None,\n",
              "                                         max_cat_threshold=None,\n",
              "                                         max_cat_to_onehot=None,\n",
              "                                         max_delta_step=None, max_depth=None,\n",
              "                                         max_leaves=None, min_child_weight=None,\n",
              "                                         missing=nan, monotone_constraints=None,\n",
              "                                         n_estimators=100, n_jobs=None,\n",
              "                                         num_parallel_tree=None, predictor=None,\n",
              "                                         random_state=None, ...),\n",
              "                require_dense=[True, True])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">ClassifierChain</label><div class=\"sk-toggleable__content\"><pre>ClassifierChain(classifier=XGBClassifier(base_score=None, booster=None,\n",
              "                                         callbacks=None, colsample_bylevel=None,\n",
              "                                         colsample_bynode=None,\n",
              "                                         colsample_bytree=None,\n",
              "                                         early_stopping_rounds=None,\n",
              "                                         enable_categorical=False,\n",
              "                                         eval_metric=None, feature_types=None,\n",
              "                                         gamma=None, gpu_id=None,\n",
              "                                         grow_policy=None, importance_type=None,\n",
              "                                         interaction_constraints=None,\n",
              "                                         learning_rate=None, max_bin=None,\n",
              "                                         max_cat_threshold=None,\n",
              "                                         max_cat_to_onehot=None,\n",
              "                                         max_delta_step=None, max_depth=None,\n",
              "                                         max_leaves=None, min_child_weight=None,\n",
              "                                         missing=nan, monotone_constraints=None,\n",
              "                                         n_estimators=100, n_jobs=None,\n",
              "                                         num_parallel_tree=None, predictor=None,\n",
              "                                         random_state=None, ...),\n",
              "                require_dense=[True, True])</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">classifier: XGBClassifier</label><div class=\"sk-toggleable__content\"><pre>XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
              "              colsample_bylevel=None, colsample_bynode=None,\n",
              "              colsample_bytree=None, early_stopping_rounds=None,\n",
              "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
              "              gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n",
              "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
              "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
              "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
              "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
              "              n_estimators=100, n_jobs=None, num_parallel_tree=None,\n",
              "              predictor=None, random_state=None, ...)</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">XGBClassifier</label><div class=\"sk-toggleable__content\"><pre>XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
              "              colsample_bylevel=None, colsample_bynode=None,\n",
              "              colsample_bytree=None, early_stopping_rounds=None,\n",
              "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
              "              gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n",
              "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
              "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
              "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
              "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
              "              n_estimators=100, n_jobs=None, num_parallel_tree=None,\n",
              "              predictor=None, random_state=None, ...)</pre></div></div></div></div></div></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iVs80gc3OHeu",
        "outputId": "f5f58cb9-55ae-4ac0-9449-233e5e21725f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hamming Loss (XGBoost): 0.038662486938349006\n",
            "Log Loss (XGBoost): 0.6169338468618949\n",
            "Accuracy (XGBoost): 0.8683385579937304\n"
          ]
        }
      ],
      "source": [
        "# Make predictions on the test set\n",
        "y_pred_xgb = cc_classifier_xgb.predict(X_test)\n",
        "y_pred = cc_classifier_xgb.predict_proba(X_test)\n",
        "# Calculate Hamming loss\n",
        "hamming_loss_xgb = hamming_loss(y_test, y_pred_xgb)\n",
        "print(\"Hamming Loss (XGBoost):\", hamming_loss_xgb)\n",
        "\n",
        "# Calculate log loss\n",
        "log_loss_xgb = log_loss(y_test, y_pred.toarray())\n",
        "print(\"Log Loss (XGBoost):\", log_loss_xgb)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy_xgb = accuracy_score(y_test, y_pred_xgb)\n",
        "print(\"Accuracy (XGBoost):\", accuracy_xgb)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "By both the ways(binary relevance & classifier chain, logistic regression outperformed other models in terms of hamming loss and log loss. We kept hamming loss as an evaluation metric because we knew the dataset is not balanced)"
      ],
      "metadata": {
        "id": "gczv-7iQjmNn"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xauOmzrDPEsU"
      },
      "source": [
        "Comparing results with Binary Relavance, Classifier chain metrics have been improved very slightly might be classifiers is able to preserve the correlation between the features and performing well with our data-set\n",
        "\n",
        "In Classifier chain also the Logistic regression and XGBoost classifier results seems to be and high and very close to each other."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e4gG9PycPOFj"
      },
      "source": [
        "Label Power Set Approach is also good and preserves the correlation/dependencies between the features. But only disadvantage is that, as the no:of target labels increases, the unique class labels to be mapped is also increased exponentially. And training a such huge multi-class classification problem becomes much more complex and results would be with lower accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wFvq5247Rhj-"
      },
      "source": [
        "# Conclusion\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OjMKIPFoRfuC"
      },
      "source": [
        "There are two primary approaches commonly employed to address multi-label classification problems: problem transformation methods and algorithm adaptation methods.\n",
        "\n",
        "Problem transformation methods involve converting the multi-label problem into multiple binary classification problems. This enables the use of single-class classifiers to handle each transformed problem separately. I have mainly implemented Problem transformation methods in this notebook\n",
        "\n",
        "On the other hand, algorithm adaptation methods focus on modifying the algorithms to directly handle multi-label classification. Instead of simplifying the problem by conversion, these methods aim to tackle the problem in its original, comprehensive form.\n",
        "\n",
        "However, it is worth noting that these methods require a significant amount of time to process the dataset. Therefore, to mitigate this issue, experimentation was conducted through Problem transformation methods  on a random subset of the training data."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}